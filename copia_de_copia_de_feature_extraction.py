# -*- coding: utf-8 -*-
"""Copia de Copia de Feature_Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CsrZhbNEro8kbhoV0ejq0oS4Ode9-Nb6
"""

# Import and dependency installation
# Import these when running in a notebook environment: 
# '''
# !pip install numpy pandas nltk scikit-learn textblob spacy torch transformers matplotlib
# !python -m spacy download en_core_web_sm
# !pip install PyPDF2 pdfplumber datasets
# '''

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import numpy as np
import pandas as pd
from collections import Counter
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from textblob import TextBlob
import spacy
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from scipy.ndimage import gaussian_filter1d
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import hf_hub_download
import json
import difflib
from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter
import os
import matplotlib.pyplot as plt
import pdfplumber
import streamlit as st

# For finetuning
from transformers import Trainer, TrainingArguments
# from datasets import Dataset
import torch.nn.functional as F
import random

# Optional: download model for offline use
# from huggingface_hub import snapshot_download
# snapshot_download("joeddav/distilbert-base-uncased-go-emotions-student")

# Paste the entire content of feature_extraction.py here

class BookFeatureExtractor:
    """
    Extracts emotional and thematic features from books for recommendation systems.
    Implements the methodology described in the project documentation.
    """

    def __init__(self, emotion_lexicon_path=None, model_path=None):
        """
        Initialize the feature extractor with necessary resources.

        Args:
            emotion_lexicon_path (str): Path to the emotion lexicon file
            model_path (str): Path to pre-trained or fine-tuned BERT model
        """
        # Initialize NLP tools
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))

        # Load emotion lexicon (sample format: word,emotion1,emotion2)
        self.emotion_lexicon = self._load_emotion_lexicon(emotion_lexicon_path)

        # Load NER model for character detection
        self.nlp = spacy.load("en_core_web_sm")
        self.nlp.max_length = 3_500_000

        # Load BERT model for emotion classification
        if model_path:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        else:
            # Default to pre-trained model if no fine-tuned model is provided
            self.tokenizer = AutoTokenizer.from_pretrained("joeddav/distilbert-base-uncased-go-emotions-student")
            self.model = AutoModelForSequenceClassification.from_pretrained("joeddav/distilbert-base-uncased-go-emotions-student")

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

        # Define Plutchik's 8 emotions
        self.plutchik_emotions = [
            "joy", "trust", "fear", "surprise",
            "sadness", "disgust", "anger", "anticipation"
        ]
        
        # Expanded emotion labels
        # Includes original GoEmotions labels plus literary-specific emotions
        emotion_labels = [
            # Original GoEmotions labels
            'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',
            'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',
            'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',
            'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',
            'relief', 'remorse', 'sadness', 'surprise', 'neutral',
            
            # Additional literary emotions
            'melancholy', 'longing', 'nostalgia', 'wonder', 'awe', 'hope', 
            'despair', 'loneliness', 'jealousy', 'envy', 'contempt', 'bitterness',
            'yearning', 'passion', 'serenity', 'ecstasy', 'guilt', 'shame',
            'compassion', 'empathy', 'reverence', 'resentment', 'indignation',
            'regret', 'betrayal', 'vulnerability', 'suspense', 'dread',
            'euphoria', 'disillusionment', 'whimsy', 'boredom', 'apathy', 'angst'
        ]

        self.emotion_labels = emotion_labels
        
        # Store the base model name for fine-tuning
        self.base_model_name = "joeddav/distilbert-base-uncased-go-emotions-student"

    def finetune_model(self, texts, emotion_labels, output_dir="finetuned_model", training_steps=500, batch_size=8):
        """
        Fine-tune the emotion model on literary examples.
        
        Args:
            texts (list): List of text examples (sentences or paragraphs)
            emotion_labels (list): List of emotion label dictionaries for each text
            output_dir (str): Directory to save the fine-tuned model
            training_steps (int): Number of training steps
            batch_size (int): Batch size for training
            
        Returns:
            None (saves model to disk)
        """
        print(f"Fine-tuning model on {len(texts)} examples...")
        
        # Create dataset
        train_dataset = self._prepare_dataset(texts, emotion_labels)
        
        # Set up training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=100,
            learning_rate=5e-5,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            save_strategy="epoch",
        )
        
        # Initialize trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
        )
        
        # Fine-tune model
        trainer.train()
        
        # Save fine-tuned model
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        st.text(f"Model fine-tuned and saved to {output_dir}")
        
        # Update the current model
        self.model = AutoModelForSequenceClassification.from_pretrained(output_dir)
        self.tokenizer = AutoTokenizer.from_pretrained(output_dir)
        self.model.to(self.device)
    
    def _prepare_dataset(self, texts, emotion_labels):
        """
        Prepare dataset for fine-tuning.
        
        Args:
            texts (list): List of text examples
            emotion_labels (list): List of emotion label dictionaries for each text
            
        Returns:
            Dataset: HuggingFace dataset for training
        """
        # Convert emotion dictionaries to lists matching model output size
        encoded_labels = []
        for label_dict in emotion_labels:
            # Convert dictionary to list in the same order as self.emotion_labels
            label_values = [float(label_dict.get(emotion, 0.0)) for emotion in self.emotion_labels]
            encoded_labels.append(label_values)
            
        # Create dataset
        dataset_dict = {
            "text": texts,
            "labels": encoded_labels
        }
        
        return Dataset.from_dict(dataset_dict)
    
    def generate_synthetic_training_data(self, literary_texts, sample_count=100):
        """
        Generate synthetic training data for fine-tuning from literary texts.
        
        Args:
            literary_texts (list): List of literary text segments
            sample_count (int): Number of training examples to generate
            
        Returns:
            tuple: (texts, emotion_labels) for fine-tuning
        """
        # Extract sentences from texts
        all_sentences = []
        for text in literary_texts:
            sentences = sent_tokenize1(text)
            all_sentences.extend(sentences)
            
        # Filter sentences by length
        filtered_sentences = [s for s in all_sentences if 20 <= len(s) <= 500]
        
        # Ensure we have enough sentences
        if len(filtered_sentences) < sample_count:
            # If not enough unique sentences, allow duplicates with different labels
            filtered_sentences = filtered_sentences * (sample_count // len(filtered_sentences) + 1)
            
        # Select random samples
        sampled_sentences = random.sample(filtered_sentences, min(sample_count, len(filtered_sentences)))
        
        # Generate synthetic emotion labels
        # Get base predictions from current model
        emotion_labels = []
        
        for sentence in sampled_sentences:
            # Get base prediction
            base_emotions = self.extract_bert_emotions(sentence)
            
            # Add some random adjustments to create variation in the training data
            adjusted_emotions = {}
            for emotion in self.emotion_labels:
                base_value = base_emotions.get(emotion, 0.0)
                
                # Random adjustment
                if random.random() < 0.3:  # 30% chance of significant adjustment
                    adjusted_value = base_value + random.uniform(-0.2, 0.2)
                else:
                    adjusted_value = base_value + random.uniform(-0.05, 0.05)
                    
                # Ensure value is between 0 and 1
                adjusted_value = max(0.0, min(1.0, adjusted_value))
                
                # Add to dictionary
                adjusted_emotions[emotion] = adjusted_value
                
            emotion_labels.append(adjusted_emotions)
            
        return sampled_sentences, emotion_labels
        
    def expand_model_for_new_emotions(self, output_dir="expanded_emotion_model"):
        """
        Expand the existing model to handle additional emotions.
        
        Args:
            output_dir (str): Directory to save the expanded model
            
        Returns:
            None (updates self.model in place)
        """
        print("Expanding model for new emotions...")
        
        # Get the existing model configuration
        config = self.model.config
        
        # Store the old number of labels
        old_num_labels = config.num_labels
        
        # Update the number of labels to match our expanded emotion set
        config.num_labels = len(self.emotion_labels)
        
        # Create a new model with the updated config
        new_model = AutoModelForSequenceClassification.from_pretrained(
            self.base_model_name,
            config=config
        )
        
        # Copy the weights from the old model for the original emotions
        with torch.no_grad():
            # Copy classifier weights for existing emotions
            new_model.classifier.weight.data[:old_num_labels] = self.model.classifier.weight.data
            new_model.classifier.bias.data[:old_num_labels] = self.model.classifier.bias.data
            
            # Initialize new emotion weights with similar statistical properties
            mean_weight = torch.mean(self.model.classifier.weight.data)
            std_weight = torch.std(self.model.classifier.weight.data)
            
            # Initialize new weights with similar distribution
            for i in range(old_num_labels, len(self.emotion_labels)):
                new_model.classifier.weight.data[i] = torch.randn_like(
                    new_model.classifier.weight.data[i]
                ) * std_weight + mean_weight
                
                # Initialize bias with small values
                new_model.classifier.bias.data[i] = torch.randn(1) * 0.01
        
        # Save the expanded model
        new_model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        # Update the current model
        self.model = new_model
        self.model.to(self.device)
        
        print(f"Model expanded and saved to {output_dir}")
    
    def _load_emotion_lexicon(self, lexicon_path):
        """
        Load emotion lexicon from file.

        Args:
            lexicon_path (str): Path to lexicon file

        Returns:
            dict: Word to emotions mapping
        """
        if not lexicon_path:
            # Return a small sample lexicon for testing
            return {
                "happy": ["joy"], "sad": ["sadness"],
                "angry": ["anger"], "afraid": ["fear"]
            }

        lexicon = {}
        with open(lexicon_path, 'r') as file:
            for line in file:
                parts = line.strip().split(',')
                word = parts[0]
                emotions = parts[1:]
                lexicon[word] = emotions

        return lexicon

    def clean_gutenberg_text(self, text):
        """
        Remove Project Gutenberg's header and footer boilerplate, accounting for variations.

        Args:
            text (str): Raw book text

        Returns:
            str: Cleaned book text
        """
        import re

        # Define robust patterns to detect start and end
        start_pattern = re.compile(r'\*+\s*START OF (THE|THIS) PROJECT GUTENBERG EBOOK.*', re.IGNORECASE)
        end_pattern = re.compile(r'\*+\s*END OF (THE|THIS) PROJECT GUTENBERG EBOOK.*', re.IGNORECASE)

        lines = text.splitlines()
        start_idx, end_idx = 0, len(lines)

        for i, line in enumerate(lines):
            if start_pattern.search(line):
                start_idx = i + 1  # skip the matched line
                break

        for i in range(len(lines)-1, -1, -1):
            if end_pattern.search(lines[i]):
                end_idx = i
                break

        return "\n".join(lines[start_idx:end_idx]).strip()

    def preprocess_text(self, text):
        """
        Clean and preprocess text for analysis.

        Args:
            text (str): Raw text content

        Returns:
            list: Processed tokens
            list: Processed sentences
        """
        # Convert to lowercase
        text = text.lower()

        # Remove non-alphanumeric characters
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

        # Tokenize into sentences
        sentences = sent_tokenize1(text)

        # Tokenize and lemmatize words, remove stopwords
        processed_tokens = []
        for sentence in sentences:
            tokens = [token.text for token in self.nlp(sentence)]
            tokens = [self.lemmatizer.lemmatize(token) for token in tokens
                     if token not in self.stop_words]
            processed_tokens.extend(tokens)

        return processed_tokens, sentences

    def segment_by_chapter(self, text):
        """
        Split book text into chapters.

        Args:
            text (str): Full book text

        Returns:
            list: List of chapters
        """
        # Match common chapter patterns
        chapter_patterns = [
            r'Chapter \d+', r'CHAPTER \d+',
            r'Chapter [IVXLCDM]+', r'CHAPTER [IVXLCDM]+'
        ]

        pattern = '|'.join(chapter_patterns)
        chapters = re.split(pattern, text)

        # Remove empty chapters and trim whitespace
        chapters = [chapter.strip() for chapter in chapters if chapter.strip()]

        return chapters

    def extract_lexicon_based_emotions(self, tokens):
        """
        Extract emotions based on lexicon matching.

        Args:
            tokens (list): Processed word tokens

        Returns:
            dict: Emotion counts and normalized scores
        """
        emotion_counts = {emotion: 0 for emotion in self.emotion_labels}

        for token in tokens:
            if token in self.emotion_lexicon:
                for emotion in self.emotion_lexicon[token]:
                    if emotion in emotion_counts:
                        emotion_counts[emotion] += 1

        # Normalize counts
        total = sum(emotion_counts.values())
        emotion_scores = emotion_counts.copy()

        if total > 0:
            for emotion in emotion_scores:
                emotion_scores[emotion] = emotion_scores[emotion] / total

        return {
            "counts": emotion_counts,
            "scores": emotion_scores
        }

    def chunk_sentences(self, sentences, chunk_size=5):
      """Yield successive chunks from list of sentences."""
      for i in range(0, len(sentences), chunk_size):
          yield sentences[i:i + chunk_size]

    def extract_bert_emotions(self, text, batch_size=4):
        """
        Use fine-tuned BERT to classify text into emotions.

        Args:
            text (str): Text to analyze
            batch_size (int): Batch size for processing

        Returns:
            dict: Emotion classification results
        """
        sentences = sent_tokenize1(text)
        results = {emotion: 0 for emotion in self.emotion_labels}
        all_probs = []

        # Process in batches
        for batch in self.chunk_sentences(sentences, chunk_size=5):
            inputs = self.tokenizer(
                batch,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)
            all_probs.append(probs.cpu())
            
        if not all_probs:
            return {label: 0.0 for label in self.emotion_labels}

        all_probs_tensor = torch.cat(all_probs, dim=0)
        avg_probs = torch.mean(all_probs_tensor, dim=0).numpy()
        
        # Handle the case where the model outputs fewer emotions than our expanded list
        num_model_labels = avg_probs.shape[0]
        emotion_scores = {}
        
        # First, fill in values for emotions the model knows about
        for i, emotion in enumerate(self.emotion_labels[:num_model_labels]):
            emotion_scores[emotion] = float(avg_probs[i])
            
        # For any remaining emotions in our expanded list, initialize with zeros
        # These will be updated once the model is expanded and fine-tuned
        for emotion in self.emotion_labels[num_model_labels:]:
            emotion_scores[emotion] = 0.0

        return emotion_scores

    def extract_polarity_with_textblob(self, sentences):
        """
        Extract sentiment polarity using TextBlob.

        Args:
            sentences (list): List of sentences

        Returns:
            list: Polarity scores for each sentence
        """
        polarity_scores = []

        for sentence in sentences:
            analysis = TextBlob(sentence)
            polarity_scores.append(analysis.sentiment.polarity)

        return polarity_scores

    def extract_character_emotions(self, text):
        """
        Extract emotions associated with main characters using BERT.

        Args:
            text (str): Book text

        Returns:
            dict: Character emotions mapping
        """
        paragraphs = text.split('\n\n')
        mention_counts = {}
        character_contexts = {}

        for paragraph in paragraphs:
            if len(paragraph.strip()) == 0 or len(paragraph) > 100000:
                continue

            try:
                doc = self.nlp(paragraph)
            except Exception as e:
                st.text(f"[WARNING] Skipped paragraph due to parsing error: {e}")
                continue

            for ent in doc.ents:
                if ent.label_ == "PERSON":
                    name = ent.text.strip()
                    mention_counts[name] = mention_counts.get(name, 0) + 1
                    character_contexts.setdefault(name, []).append(paragraph)


        #trial
        merged = {}
        for name in character_contexts:
            found = False
            for canon in merged:
                if difflib.SequenceMatcher(None, name, canon).ratio() > 0.85:
                    merged[canon].extend(character_contexts[name])
                    found = True
                    break
            if not found:
                merged[name] = character_contexts[name]


        # Get top 10 most mentioned characters
        top_characters = sorted(mention_counts.items(), key=lambda x: x[1], reverse=True)[:10]

        character_emotions = {}
        for name, _ in top_characters:
            context_text = " ".join(character_contexts[name])
            if context_text.strip():
                emotion_scores = self.extract_bert_emotions(context_text)
                character_emotions[name] = emotion_scores

        return character_emotions

    def get_custom_stopwords_from_entities(self, text, nlp, top_n=5):
          doc = nlp(text)
          people = [ent.text.lower() for ent in doc.ents if ent.label_ == "PERSON"]
          most_common = [name for name, _ in Counter(people).most_common(top_n)]
          return set(most_common)


    def extract_tfidf_features(self, chapters):
        """
        Extract TF-IDF features from book chapters.

        Args:
            chapters (list): List of book chapters

        Returns:
            array: TF-IDF matrix
            list: Feature names
        """

                # Get common character names dynamically
        full_text = " ".join(chapters)
        custom_stopwords = self.get_custom_stopwords_from_entities(full_text, self.nlp)

        processed_chapters = []
        for chapter in chapters:
            # Lowercase
            chapter = chapter.lower()
            # Remove non-alphabetic characters
            chapter = re.sub(r'[^a-zA-Z\s]', ' ', chapter)

            # Tokenize with spaCy
            tokens = [token for token in self.nlp(chapter)]

            # Filter: keep nouns and adjectives, remove verbs, stopwords, and named entities
            filtered_tokens = [
                self.lemmatizer.lemmatize(token.text.lower())
                for token in tokens
                if token.pos_ in {"NOUN", "ADJ"}
                and token.text.lower() not in self.stop_words
                and token.ent_type_ != "PERSON"
                and token.text.lower() not in custom_stopwords
            ]

            cleaned = " ".join(filtered_tokens)
            processed_chapters.append(cleaned)

        # Apply CountVectorizer with bigram support and improved stopword filtering
        vectorizer = CountVectorizer(
            max_df=0.8,
            min_df=5,
            stop_words='english',
            ngram_range=(1, 2)  # unigrams and bigrams
        )
        tfidf_matrix = vectorizer.fit_transform(processed_chapters)

        return tfidf_matrix, vectorizer.get_feature_names_out()



    def extract_topics_with_lda(self, doc_term_matrix, feature_names, num_topics=8):
      lda = LatentDirichletAllocation(n_components=num_topics, max_iter=15, learning_method='online', random_state=42)
      lda.fit(doc_term_matrix)

      topics = {}
      for topic_idx, topic in enumerate(lda.components_):
          top_words_idx = topic.argsort()[:-11:-1]
          top_words = [feature_names[i] for i in top_words_idx]
          topics[f"Topic {topic_idx}"] = top_words

      doc_topic_dist = lda.transform(doc_term_matrix)

      return topics, doc_topic_dist


    def extract_topics_with_lda(self, tfidf_matrix, feature_names, num_topics=10):
        """
        Extract topics using Latent Dirichlet Allocation.

        Args:
            tfidf_matrix: TF-IDF matrix
            feature_names: Feature names from the vectorizer
            num_topics (int): Number of topics to extract

        Returns:
            dict: Topics with top words
            array: Document-topic distribution
        """
        lda = LatentDirichletAllocation(n_components=num_topics, max_iter=15, learning_method='online', random_state=42)
        lda.fit(tfidf_matrix)

        topics = {}
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[:-11:-1]  # Get top 10 words
            top_words = [feature_names[i] for i in top_words_idx]
            topics[f"topic_{topic_idx}"] = top_words

        # Get document-topic distribution
        doc_topic_dist = lda.transform(tfidf_matrix)

        return topics, doc_topic_dist

    def create_sentiment_trajectory(self, chapters):
        """
        Create a sentiment trajectory for the book.

        Args:
            chapters (list): List of book chapters

        Returns:
            dict: Sentiment trajectory data
        """
        trajectory = {
            "polarity": [],
            "emotions": {emotion: [] for emotion in self.emotion_labels},
            "chapter_boundaries": []
        }

        current_position = 0

        for chapter in chapters:
            tokens, sentences = self.preprocess_text(chapter)

            # Get polarity
            polarity_scores = self.extract_polarity_with_textblob(sentences)
            avg_polarity = sum(polarity_scores) / len(polarity_scores) if polarity_scores else 0
            trajectory["polarity"].append(avg_polarity)

            # Get emotions
            #emotions = self.extract_lexicon_based_emotions(tokens)
            chapter_text = " ".join(sentences)
            emotions = self.extract_bert_emotions(chapter_text)

            for emotion in self.emotion_labels:
                #trajectory["emotions"][emotion].append(emotions["scores"].get(emotion, 0))
                trajectory["emotions"][emotion].append(emotions.get(emotion, 0))

            # Track chapter boundary
            current_position += len(sentences)
            trajectory["chapter_boundaries"].append(current_position)

        return trajectory

    def apply_rolling_average(self, trajectory, window=3):
        """
        Apply rolling average smoothing to the sentiment trajectory.

        Args:
            trajectory (dict): Sentiment trajectory
            window (int): Window size for rolling average

        Returns:
            dict: Smoothed trajectory
        """
        smoothed = trajectory.copy()

        # Smooth polarity
        polarity = np.array(trajectory["polarity"])
        smoothed["polarity"] = np.convolve(polarity, np.ones(window)/window, mode='valid').tolist()

        # Smooth emotions
        for emotion in self.emotion_labels:
            values = np.array(trajectory["emotions"][emotion])
            smoothed["emotions"][emotion] = np.convolve(values, np.ones(window)/window, mode='valid').tolist()

        # Adjust chapter boundaries
        offset = (window - 1) // 2
        smoothed["chapter_boundaries"] = trajectory["chapter_boundaries"][offset:len(smoothed["polarity"])+offset]

        return smoothed
    from scipy.ndimage import gaussian_filter1d

    def apply_gaussian_smoothing(self, trajectory, sigma=1):
        """
        Apply Gaussian smoothing to the sentiment trajectory.

        Args:
            trajectory (dict): Sentiment trajectory
            sigma (int): Standard deviation for Gaussian kernel

        Returns:
            dict: Smoothed trajectory
        """
        smoothed = {
            "polarity": [],
            "emotions": {emotion: [] for emotion in self.emotion_labels},
            "chapter_boundaries": trajectory["chapter_boundaries"]  # optional: leave as-is
        }

        # Smooth polarity
        polarity_array = np.array(trajectory["polarity"])
        smoothed["polarity"] = gaussian_filter1d(polarity_array, sigma=sigma).tolist()

        # Smooth emotions
        for emotion in self.emotion_labels:
            values = np.array(trajectory["emotions"][emotion])
            smoothed["emotions"][emotion] = gaussian_filter1d(values, sigma=sigma).tolist()

        return smoothed


    def extract_book_profile(self, text):
        """
        Extract a comprehensive book profile with all features.

        Args:
            text (str): Book text

        Returns:
            dict: Book profile with emotional and thematic features
        """
        # Clean Gutenberg metadata
        text = self.clean_gutenberg_text(text)

        # Segment book into chapters
        chapters = self.segment_by_chapter(text)

        # Process full text
        all_tokens, all_sentences = self.preprocess_text(text)

        # Extract overall emotion profile
        emotion_profile = self.extract_lexicon_based_emotions(all_tokens)

        # Extract sentiment trajectory
        trajectory = self.create_sentiment_trajectory(chapters)

        # Apply smoothing
        #smoothed_trajectory = self.apply_rolling_average(trajectory)
        smoothed_trajectory = self.apply_gaussian_smoothing(trajectory, sigma=1)

        # Extract TF-IDF features and topics
        tfidf_matrix, feature_names = self.extract_tfidf_features(chapters)
        topics, doc_topic_dist = self.extract_topics_with_lda(tfidf_matrix, feature_names)

        # Extract character emotions
        character_emotions = self.extract_character_emotions(text)

        # Compile book profile
        book_profile = {
            "overall_emotions": emotion_profile["scores"],
            "sentiment_trajectory": {
                "raw": trajectory,
                "smoothed": smoothed_trajectory
            },
            "topics": topics,
            "chapter_topic_distribution": doc_topic_dist.tolist(),
            "character_emotions": character_emotions
        }

        return book_profile

def sent_tokenize1(text):
    # Basic sentence splitter using punctuation (approximate)
    return [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if s.strip()]

# Paste the entire content of example_analysis.py here, but remove the if __name__ == "__main__" block at the end
def load_book(book_path):
    """Load a book from file."""
    with open(book_path, 'r', encoding='utf-8', errors='ignore') as f:
        return f.read()

def load_book_pdf(book_path):
    if book_path.lower().endswith(".pdf"):
        text = ""
        with pdfplumber.open(book_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    else:
        with open(book_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()


def save_profile(profile, output_path):
    """Save the extracted profile as JSON."""
    with open(output_path, 'w') as f:
        json.dump(profile, f, indent=2)
    print(f"Book profile saved to {output_path}")

def plot_sentiment_trajectory(profile, output_dir):
    """Plot the sentiment trajectory of the book."""
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Get data
    trajectory = profile["sentiment_trajectory"]["smoothed"]
    polarity = trajectory["polarity"]
    emotions = trajectory["emotions"]

    # Plot polarity
    plt.figure(figsize=(12, 6))
    plt.plot(polarity)
    plt.title("Sentiment Polarity Throughout the Book")
    plt.xlabel("Chapter")
    plt.ylabel("Polarity (-1 to 1)")
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(output_dir, "polarity_trajectory.png"))
    plt.close()

    # Plot top N emotions
    N = 10
    avg_intensity = {e: np.mean(v) for e, v in emotions.items() if len(v) > 0}
    top_emotions = sorted(avg_intensity.items(), key=lambda x: x[1], reverse=True)[:N]
    top_emotions = {e: emotions[e] for e, _ in top_emotions}

    plt.figure(figsize=(12, 8))
    for emotion, values in top_emotions.items():
        plt.plot(values, label=emotion.capitalize())

    plt.title("Top Emotional Trajectory Throughout the Book")
    plt.xlabel("Chapter")
    plt.ylabel("Emotion Intensity")
    plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
    plt.tight_layout()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(output_dir, "emotion_trajectory.png"))
    plt.close()

    # Create stacked area chart for emotional composition
    plt.figure(figsize=(14, 8))
    used_emotions = {e: vals for e, vals in emotions.items() if np.any(np.array(vals) > 0)}

    if used_emotions:
        avg_intensities = {e: np.mean(vals) for e, vals in used_emotions.items()}
        top_n = 10
        top_emotions = sorted(avg_intensities.items(), key=lambda x: x[1], reverse=True)[:top_n]
        selected_emotions = [e for e, _ in top_emotions]
        emotions_array = np.array([used_emotions[e] for e in selected_emotions])
        labels = [e.capitalize() for e in selected_emotions]

        # Plot
        plt.stackplot(range(len(emotions_array[0])), emotions_array, labels=labels, alpha=0.8)
        plt.title("Emotional Composition Throughout the Book")
        plt.xlabel("Chapter")
        plt.ylabel("Proportion")
        plt.legend(loc="upper left", fontsize=9)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "emotion_composition.png"))
        plt.close()
    
    # Plot literary-specific emotions (if available)
    literary_emotions = [
        'melancholy', 'longing', 'nostalgia', 'wonder', 'awe', 
        'hope', 'despair', 'loneliness', 'yearning', 'passion'
    ]
    
    # Filter to those present in the data with non-zero values
    available_literary = {e: emotions[e] for e in literary_emotions 
                         if e in emotions and np.any(np.array(emotions[e]) > 0)}
    
    if available_literary:
        plt.figure(figsize=(12, 8))
        for emotion, values in available_literary.items():
            plt.plot(values, label=emotion.capitalize())

        plt.title("Literary Emotions Throughout the Book")
        plt.xlabel("Chapter")
        plt.ylabel("Emotion Intensity")
        plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
        plt.tight_layout()
        plt.grid(True, alpha=0.3)
        plt.savefig(os.path.join(output_dir, "literary_emotion_trajectory.png"))
        plt.close()
    
    print(f"Plots saved to {output_dir}")

def plot_character_emotions(profile, output_dir):
    """Plot the emotions associated with main characters."""
    character_emotions = profile["character_emotions"]
    if not character_emotions:
        print("No character emotions to plot")
        return

    # Get top characters by total emotion intensity
    top_chars = sorted(
        character_emotions.items(),
        key=lambda x: sum(x[1].values()),
        reverse=True
    )[:5]  # Top 5 characters

    # Create a separate plot for each character with their top emotions
    for char, char_emotions in top_chars:
        # Get top emotions for this character
        top_emotions = sorted(
            char_emotions.items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]  # Top 10 emotions
        
        emotions = [e for e, _ in top_emotions]
        values = [v for _, v in top_emotions]
        
        plt.figure(figsize=(10, 6))
        bars = plt.barh(emotions, values, color='skyblue')
        
        # Add values to the end of each bar
        for bar in bars:
            width = bar.get_width()
            plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{width:.2f}', ha='left', va='center')
        
        plt.title(f"Top Emotions for {char}")
        plt.xlabel("Intensity")
        plt.ylabel("Emotion")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"character_{char.replace(' ', '_')}.png"))
        plt.close()
    
    # Create comparison plot for all top characters and their common top emotions
    # Find common top emotions across characters
    all_emotions = set()
    for _, char_emotions in top_chars:
        top_char_emotions = sorted(
            char_emotions.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]  # Top 5 emotions per character
        all_emotions.update([e for e, _ in top_char_emotions])
    
    # Select the top N overall emotions for comparison
    all_emotion_values = {}
    for emotion in all_emotions:
        all_emotion_values[emotion] = sum(
            char_emotions.get(emotion, 0) 
            for _, char_emotions in top_chars
        )
    
    top_comparison_emotions = sorted(
        all_emotion_values.items(),
        key=lambda x: x[1],
        reverse=True
    )[:8]  # Top 8 emotions overall
    comparison_emotions = [e for e, _ in top_comparison_emotions]
    
    # Create the bar chart
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # Set width of bars
    barWidth = 0.15
    
    # Set positions on X axis
    r = np.arange(len(comparison_emotions))
    
    # Create bars for each character
    for i, (char, char_emotions) in enumerate(top_chars):
        emotion_values = [char_emotions.get(e, 0) for e in comparison_emotions]
        ax.bar(r + i * barWidth, emotion_values, width=barWidth, label=char)
    
    # Add labels
    plt.xlabel('Emotions', fontweight='bold')
    plt.ylabel('Intensity', fontweight='bold')
    plt.title('Emotions Associated with Main Characters')
    plt.xticks([r + barWidth * 2 for r in range(len(comparison_emotions))], 
              [e.capitalize() for e in comparison_emotions], 
              rotation=45)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "character_emotions_comparison.png"))
    plt.close()
    
    print(f"Character emotions plots saved to {output_dir}")

def plot_topic_heatmap(profile, output_dir):
    """Plot a heatmap of topics by chapter."""
    topic_dist = np.array(profile["chapter_topic_distribution"])
    topics = profile["topics"]

    # Number of chapters and topics
    n_chapters, n_topics = topic_dist.shape

    # Create labels for topics
    topic_labels = []
    for i in range(n_topics):
        top_words = topics[f"topic_{i}"][:3]  # Get top 3 words
        topic_labels.append(f"Topic {i}: {', '.join(top_words)}")

    # Create the heatmap
    plt.figure(figsize=(18, 10))
    plt.imshow(topic_dist, cmap='YlOrRd')
    plt.colorbar(label='Topic Probability')

    # Add labels
    plt.yticks(range(n_chapters), [f"Chapter {i+1}" for i in range(n_chapters)])
    plt.xticks(range(n_topics), topic_labels, rotation=45, ha='right')

    plt.title('Topic Distribution Across Chapters')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "topic_heatmap.png"))
    plt.close()
    
    print(f"Topic heatmap saved to {output_dir}")

def print_profile_summary(profile):
    """Print a summary of the book profile with expanded emotions."""
    st.text("\nðŸ“˜ Profile Summary:")

    # Basic emotion categories
    basic_emotions = [
        'joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 
        'trust', 'anticipation', 'neutral'
    ]
    
    # Literary-specific emotions
    literary_emotions = [
        'melancholy', 'longing', 'nostalgia', 'wonder', 'awe', 'hope', 
        'despair', 'loneliness', 'yearning', 'passion'
    ]
    
    # Social emotions
    social_emotions = [
        'love', 'admiration', 'gratitude', 'pride', 'embarrassment',
        'guilt', 'shame', 'jealousy', 'envy'
    ]
    
    # Top overall emotions
    print("\nðŸ” Top Overall Emotions:")
    top_emotions = sorted(profile["overall_emotions"].items(), key=lambda x: x[1], reverse=True)[:8]
    for emotion, score in top_emotions:
        print(f"  - {emotion.capitalize()}: {score:.3f}")
    
    # Check if we have any literary emotions with significant presence
    literary_scores = {e: profile["overall_emotions"].get(e, 0) 
                      for e in literary_emotions 
                      if e in profile["overall_emotions"] and profile["overall_emotions"].get(e, 0) > 0.05}
    
    if literary_scores:
        print("\nðŸ“š Top Literary Emotions:")
        top_literary = sorted(literary_scores.items(), key=lambda x: x[1], reverse=True)[:5]
        for emotion, score in top_literary:
            print(f"  - {emotion.capitalize()}: {score:.3f}")

    # Character emotions
    print("\nðŸ§‘â€ðŸ¤â€ðŸ§‘ Character Emotions:")
    for name, scores in profile["character_emotions"].items():
        top_char_emotions = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:3]
        print(f"  {name}: {', '.join([f'{e} ({s:.2f})' for e, s in top_char_emotions])}")

    # Topics
    print("\nðŸ§  Main Topics:")
    for tid, words in list(profile["topics"].items())[:5]:
        print(f"  {tid}: {', '.join(words[:5])}")

    # Emotional narrative arc
    if "sentiment_trajectory" in profile and "smoothed" in profile["sentiment_trajectory"]:
        trajectory = profile["sentiment_trajectory"]["smoothed"]
        if "polarity" in trajectory and trajectory["polarity"]:
            polarity = trajectory["polarity"]
            avg_polarity = sum(polarity) / len(polarity)
            polarity_shift = polarity[-1] - polarity[0]
            
            print("\nðŸ“ˆ Narrative Arc:")
            print(f"  Average sentiment: {avg_polarity:.2f} ({avg_polarity > 0 and 'Positive' or 'Negative'})")
            print(f"  Overall shift: {polarity_shift:.2f} ({polarity_shift > 0 and 'Becomes more positive' or 'Becomes more negative'})")
            
            # Find emotional peak and valley
            peak_idx = np.argmax(polarity)
            valley_idx = np.argmin(polarity)
            print(f"  Emotional peak: Chapter {peak_idx + 1} ({polarity[peak_idx]:.2f})")
            print(f"  Emotional valley: Chapter {valley_idx + 1} ({polarity[valley_idx]:.2f})")

def analyze_book(book_path, output_dir="results", use_expanded_emotions=False, finetune=False):
    """
    Analyze a book and generate visualizations.
    
    Args:
        book_path (str): Path to the book file
        output_dir (str): Directory to save results
        use_expanded_emotions (bool): Whether to use expanded emotions set
        finetune (bool): Whether to finetune the model on the book
        
    Returns:
        dict: Book profile
    """
    print(f"Analyzing book: {book_path}")

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Load book
    book_text = load_book(book_path)
    print(f"Book loaded: {len(book_text)} characters")

    # Create extractor
    extractor = BookFeatureExtractor()
    
    # Expand emotions if requested
    if use_expanded_emotions:
        print("Expanding emotion model...")
        expanded_model_dir = os.path.join(output_dir, "expanded_model")
        extractor.expand_model_for_new_emotions(expanded_model_dir)
        
    # Finetune if requested
    if finetune:
        print("Fine-tuning model on book content...")
        # Generate training data from the book
        chapters = extractor.segment_by_chapter(book_text)
        training_chapters = chapters[:min(5, len(chapters))]  # Use up to 5 chapters
        
        print(f"Generating training data from {len(training_chapters)} chapters...")
        train_texts, train_labels = extractor.generate_synthetic_training_data(
            training_chapters, 
            sample_count=100
        )
        
        # Fine-tune
        finetuned_model_dir = os.path.join(output_dir, "finetuned_model") 
        extractor.finetune_model(
            train_texts, 
            train_labels, 
            output_dir=finetuned_model_dir,
            training_steps=200
        )

    # Extract features
    print("Extracting features...")
    profile = extractor.extract_book_profile(book_text)

    # Save profile
    profile_path = os.path.join(output_dir, "book_profile.json")
    save_profile(profile, profile_path)

    # Generate visualizations
    print("Generating visualizations...")
    plot_sentiment_trajectory(profile, output_dir)
    plot_character_emotions(profile, output_dir)
    plot_topic_heatmap(profile, output_dir)

    # Print summary
    print_profile_summary(profile)

    print("Analysis complete!")
    return profile

from google.colab import files
uploaded = files.upload()  # This will prompt you to upload your book file

import io
book_filename = list(uploaded.keys())[0]  # Get the filename of the uploaded book

# Try different encodings
encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
book_text = None

for encoding in encodings:
    try:
        book_text = uploaded[book_filename].decode(encoding)
        print(f"Successfully decoded with {encoding} encoding")
        break
    except UnicodeDecodeError:
        print(f"Failed to decode with {encoding}")

if book_text is None:
    raise Exception("Could not decode the book file with any of the attempted encodings")

# Create extractor and analyze
extractor = BookFeatureExtractor()
book_profile = extractor.extract_book_profile(book_text)

# Create visualizations
import os
output_dir = "results"
os.makedirs(output_dir, exist_ok=True)
plot_sentiment_trajectory(book_profile, output_dir)
plot_character_emotions(book_profile, output_dir)
plot_topic_heatmap(book_profile, output_dir)

print("Analysis complete!")

"""### Example of using expanded emotions and finetuning

This section demonstrates how to use the expanded emotions and fine-tuning capabilities
"""

def example_expanded_emotions_and_finetuning():
    """
    Example showing how to use the fine-tuning capabilities with sample sentences.
    This demonstrates how to train on small text samples rather than whole books.
    """
    # Sample literary texts (you would use your own dataset)
    sample_texts = [
        "The soft melody of rain against the window pane filled her with a deep melancholy, memories of childhood summers flooding back.",
        "His eyes blazed with fury as he confronted the man who had betrayed his trust.",
        "The vast expanse of stars above filled them with wonder and a profound sense of their own insignificance.",
        "She gazed longingly at the letter, her heart aching with nostalgia for a time that could never be reclaimed.",
        "The news of his friend's death left him in a state of shock, followed by a crushing wave of grief."
    ]
    
    # Create extractor
    extractor = BookFeatureExtractor()
    
    # First, expand the model to handle new emotions
    extractor.expand_model_for_new_emotions("expanded_model")
    
    # Generate synthetic training data
    print("Generating synthetic training data...")
    train_texts, train_labels = extractor.generate_synthetic_training_data(sample_texts, sample_count=20)
    
    # Fine-tune the model
    extractor.finetune_model(train_texts, train_labels, output_dir="finetuned_model", training_steps=100, batch_size=4)
    
    # Now use the fine-tuned model to analyze a text
    test_text = "The ancient castle stood silently against the twilight sky, its weathered stones telling stories of glory and tragedy, of love lost and honor gained through centuries of human drama."
    
    # Analyze emotions in the text
    emotions = extractor.extract_bert_emotions(test_text)
    
    # Print top emotions
    print("\nTop emotions detected in the test text:")
    top_emotions = sorted(emotions.items(), key=lambda x: x[1], reverse=True)[:10]
    for emotion, score in top_emotions:
        print(f"  - {emotion.capitalize()}: {score:.3f}")

# Uncomment the line below to run the example
# example_expanded_emotions_and_finetuning()

"""
HOW TO USE THE EXPANDED EMOTIONS AND FINE-TUNING:

1. EXPANDING THE EMOTION SET:
   - The model now supports 60+ emotions including literary-specific ones like melancholy, longing, wonder, etc.
   - To use the expanded emotions, you need to call `expand_model_for_new_emotions()` after creating your extractor.

2. FINE-TUNING PROCESS:
   a. Generate or collect literary text examples with emotion annotations
   b. Call `expand_model_for_new_emotions()` to enable all emotions
   c. Call `finetune_model(texts, emotion_labels)` with your training data
   d. Use the fine-tuned model for analysis with `extract_bert_emotions()` or `extract_book_profile()`

3. SYNTHETIC DATA GENERATION:
   - If you don't have annotated data, use `generate_synthetic_training_data()` with literary texts
   - This creates training examples with synthetic emotion labels based on the current model
   - Fine-tune on this data to adapt the model to literary language and new emotions

4. RECOMMENDED WORKFLOW:
   - Start with general books from your domain
   - Expand the model for new emotions
   - Generate synthetic training data from book chapters
   - Fine-tune the model
   - Analyze your target books with the improved model
"""

def example_analyze_book_with_expanded_emotions():
    """
    Example demonstrating how to analyze a complete book with expanded emotions.
    """
    # Path to your book
    book_path = "your_book.txt"  # Replace with actual path
    
    # Analyze with expanded emotions and fine-tuning
    profile = analyze_book(
        book_path=book_path,
        output_dir="results_expanded",
        use_expanded_emotions=True,
        finetune=True
    )
    
    # The profile will now contain the expanded emotions
    # and the results are saved in the output directory

# Uncomment to run the example
# example_analyze_book_with_expanded_emotions()